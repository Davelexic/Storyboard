tokenizer = AutoTokenizer.from_pretrained("cpierse/gpt2_film_scripts")
model = AutoModelForCausalLM.from_pretrained("cpierse/gpt2_film_scripts")
model.eval()
max_length = 500
num_samples = 1

output = model.generate(
            bos_token_id=random.randint(1,50000),
            do_sample=True,   
            top_k=50, 
            max_length = max_length,
            top_p=0.95, 
            num_return_sequences=num_samples)

decoded_output = []
for sample in output:
        decoded_output.append(tokenizer.decode(
            sample, skip_special_tokens=True))
print(decoded_output[0]) 
